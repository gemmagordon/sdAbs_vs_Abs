{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1187a873-6d8f-4e98-b54b-ed27c216ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script sets up the information needed to run arpeggio directly on the server. Besides, the script reads the output and stores the output as a summary per complex. \n",
    "#@Author: Henriette Capel\n",
    "#@Date: 11-04-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7a5d6f-a935-4025-9e6f-c9aec0f052b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/icarus/capel/miniconda3/envs/minor_internship/lib/python3.10/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from ABDB import database as db\n",
    "from sklearn import metrics\n",
    "from pandas.errors import EmptyDataError \n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05dba3b8-8b62-4f85-94c4-7c598067a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location files\n",
    "#/data/icarus/capel/ABDB/entries/7jmo/structure/imgt/7jmo.pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54266e92-d8cc-47fe-957b-b3499f150f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_command = \"python /data/icarus/capel/arpeggio/arpeggio.py\"\n",
    "arpeggio_output_path = \"/data/icarus/capel/data_arpeggio/\"\n",
    "folder_input = \"/data/icarus/capel/ABDB/entries/_/structure/imgt/\"\n",
    "options_command = \"-i 4.5 -he \"\n",
    "file_cleaning = \"/data/icarus/capel/pdbtools/clean_pdb.py\"\n",
    "\n",
    "columns_types_interactions = ['clash', 'covalent', 'vdw_clash', 'vdw', 'proximal', 'hbond', 'weak_hbond', 'xbond', 'ionic', 'metal_complex', 'aromatic', 'hydrophobic', 'carbonyl', 'polar', 'weak_polar']\n",
    "interaction_types = ['covalent', 'vdw', 'hbond', 'weak_hbond', 'xbond', 'ionic', 'metal_complex', 'aromatic', 'hydrophobic', 'carbonyl', 'polar', 'weak_polar'] #Check\n",
    "contact_types = ['proximal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6775ba-1119-421b-8022-ac17fe54c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def read_dataset(filename_ds):\n",
    "    \n",
    "    df = pd.read_csv(filename_ds, converters={i: str for i in range(100)})\n",
    "    \n",
    "    for colname in df.columns.values.tolist():\n",
    "        try:\n",
    "            df[colname] = [ast.literal_eval(d) for d in df[colname]]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return df\n",
    "\n",
    "#Functions creating running commands/files\n",
    "def create_sh_arpeggio_commands(df, start_command, folder_input, options_command, arpeggio_output_path, name_output_file):\n",
    "    pdb_list = df[\"pdb\"].tolist()\n",
    "    txt_string = \"\"\n",
    "    for pdb in set(pdb_list):\n",
    "        ap_command = create_arpeggio_command(df, pdb, start_command, folder_input, options_command)\n",
    "        \n",
    "        txt_string += f'echo \"{pdb}\" \\n{ap_command}\\n'\n",
    "    \n",
    "    #write to sh file\n",
    "    with open (f'{arpeggio_output_path}{name_output_file}.sh', 'w') as rsh:\n",
    "        rsh.writelines(\"#! /bin/bash \\n\")\n",
    "        rsh.writelines('echo \"Running Arpeggio for the whole dataset\"\\n')\n",
    "        rsh.writelines(txt_string)\n",
    "        rsh.writelines('echo \"Done\"\\n')\n",
    "    return None\n",
    "\n",
    "def create_py_arpeggio_commands(df, start_command, folder_input, options_command, arpeggio_output_path, name_output_file):\n",
    "    pdb_list = df[\"pdb\"].tolist()\n",
    "    txt_string = \"\"\n",
    "    for pdb in set(pdb_list):\n",
    "        ap_command = create_arpeggio_command(df, pdb, start_command, folder_input, options_command)\n",
    "\n",
    "        txt_string += f'subprocess.call(\"{ap_command}\", shell=True)\\n'\n",
    "    \n",
    "    #write to sh file\n",
    "    with open (f'{arpeggio_output_path}{name_output_file}.py', 'w') as f:\n",
    "        f.writelines('import subprocess \\n')\n",
    "        f.writelines(txt_string)\n",
    "    return None\n",
    "\n",
    "def create_arpeggio_command(df, pdb, start_command, folder_input, options_command, cleaned=False):\n",
    "    df_selected = df[df[\"pdb\"]== pdb]\n",
    "    string_interaction_info = \"\"\n",
    "    for index, row in df_selected.iterrows():\n",
    "        ag_chain = row[\"antigen_chain\"]\n",
    "        epitope_dict = row[\"epitope_num_interactions\"]\n",
    "        string_interaction_info += transform_dict_epitope(epitope_dict, ag_chain)\n",
    "    list_path_input_parts = folder_input.split(\"_\")\n",
    "    \n",
    "    if cleaned:\n",
    "        correct_path_input = f\"{list_path_input_parts[0]}{pdb}{list_path_input_parts[1]}{pdb}.clean.pdb\"\n",
    "    else:\n",
    "        correct_path_input = f\"{list_path_input_parts[0]}{pdb}{list_path_input_parts[1]}{pdb}.pdb\"\n",
    "    \n",
    "    command_line = f\"{start_command} {correct_path_input} {options_command}\"\n",
    "    \n",
    "    return command_line\n",
    "\n",
    "def create_sh_arpeggio_failed_pdbs(df, list_failed_pdbs, start_command, folder_input, options_command, file_cleaning, name_output_file):\n",
    "    #First run pdbtools/clean_pdb.py than try running areggio again\n",
    "    list_path_input_parts = folder_input.split(\"_\")\n",
    "    running_commands = \"\"\n",
    "    for pdb in list_failed_pdbs:\n",
    "        cleaning_command = f\"python {file_cleaning} {list_path_input_parts[0]}{pdb}{list_path_input_parts[1]}{pdb}.pdb\"\n",
    "        ap_command = create_arpeggio_command(df, pdb, start_command, folder_input, options_command, cleaned=True)\n",
    "        \n",
    "        running_commands += f'echo \"{pdb}\" \\n{cleaning_command} \\n{ap_command}\\n'\n",
    "    \n",
    "    with open (f'{arpeggio_output_path}{name_output_file}.sh', 'w') as rsh:\n",
    "        rsh.writelines(\"#! /bin/bash \\n\")\n",
    "        rsh.writelines('echo \"Running Arpeggio for the failed dataset\"\\n')\n",
    "        rsh.writelines(running_commands)\n",
    "        rsh.writelines('echo \"Done\"\\n')\n",
    "    return None  \n",
    "\n",
    "\n",
    "def create_py_arpeggio_failed_pdbs(df, list_failed_pdbs, start_command, folder_input, options_command, file_cleaning, name_output_file):\n",
    "    #First run pdbtools/clean_pdb.py than try running areggio again\n",
    "    list_path_input_parts = folder_input.split(\"_\")\n",
    "    running_commands = \"\"\n",
    "    for pdb in list_failed_pdbs:\n",
    "        cleaning_command = f\"python {file_cleaning} {list_path_input_parts[0]}{pdb}{list_path_input_parts[1]}{pdb}.pdb\"\n",
    "        ap_command = create_arpeggio_command(df, pdb, start_command, folder_input, options_command, cleaned=True)\n",
    "        \n",
    "        running_commands += f'subprocess.call(\"{cleaning_command}\", shell=True) \\nsubprocess.call(\"{ap_command}\", shell=True)\\n'\n",
    "    \n",
    "    with open (f'{arpeggio_output_path}{name_output_file}.py', 'w') as f:\n",
    "        f.writelines('import subprocess \\n')\n",
    "        f.writelines(running_commands)\n",
    "    return None\n",
    "    \n",
    "\n",
    "def transform_dict_epitope(epitope_interaction_dict, ag_chain):\n",
    "    string_interaction_info = \"\"\n",
    "    for residue_id in epitope_interaction_dict.keys():\n",
    "        try: \n",
    "            res_aa_name, res_num, res_ins_code = residue_id.split(\"_\")\n",
    "            string_interaction_info += f\"-s /{str(ag_chain)}/{str(res_num)}[{res_ins_code}]/ \"\n",
    "        except ValueError: #2 values to unpack if no res_ins_code\n",
    "            res_aa_name, res_num = residue_id.split(\"_\")\n",
    "            string_interaction_info += f\"-s /{str(ag_chain)}/{str(res_num)}/ \"\n",
    "    return string_interaction_info\n",
    "\n",
    "\n",
    "\n",
    "### Functions to read output arpeggio\n",
    "def read_arpeggio_result(main_path, df_input, columns_types_interactions, interaction_types_list, contact_types_list, folder, failed_try_pdb_list=False, writing_file=False):\n",
    "    \n",
    "    #This function reads the output \".contacts\" file of all complexes created by arpeggio. It selects all atom interactions between the antibody and the antigen.\n",
    "    #Next another function (store_interactions_per_residue) will be called to store the interactions per residue instead of per atom. A general summary about the interactings in one complex is stored as a row in a dataframe. \n",
    "    #Note: Run script for single domain and full length seperately\n",
    "    #Note the first run of this function will indicate pdbs that need to be cleaned before running Arpeggio. If you want to run this function after cleaning these pdbs, indicate a list of these pdbs as 7th input argument.\n",
    "    \n",
    "    #Set up empty summary dataframe\n",
    "    columns_ds_df = ['pdb', 'antigen_chain', 'antibody_chain', 'epitope_interactions', 'paratope_interactions', 'number_interactions', 'number_contacts'] + columns_types_interactions\n",
    "    df_store_interaction_per_dataset = pd.DataFrame(columns= columns_ds_df)\n",
    "    \n",
    "    index_counter_main = 0\n",
    "    count_arpeggio_failed = 0\n",
    "    failed_arpeggio_pdbs = set()\n",
    "    \n",
    "    #Check if function is run over all pdbs or only over the failed (and now cleaned) pdbs. \n",
    "    if failed_try_pdb_list:\n",
    "        pdbs_to_study = failed_try_pdb_list\n",
    "    else:\n",
    "        pdbs_to_study = df_input['pdb'].tolist()\n",
    "    \n",
    "    #Do for every pdb in the dataset \n",
    "    for pdb in set(pdbs_to_study):\n",
    "        df_selected = df_input[df_input[\"pdb\"] == pdb]\n",
    "        \n",
    "        #Check if arpeggio has created the output file\n",
    "        try: \n",
    "            if failed_try_pdb_list:\n",
    "                df_output = pd.read_csv(f\"{main_path}{pdb}.clean.contacts\", sep=\"\\t\")\n",
    "            else:\n",
    "                df_output = pd.read_csv(f\"{main_path}{pdb}.contacts\", sep=\"\\t\")\n",
    "            \n",
    "            #one pdb can contain multiple complexes. Look into each complex individualy.\n",
    "            for index, row in df_selected.iterrows():  \n",
    "                \n",
    "                #Store only interactions between our antibody and antigen\n",
    "                indexes_to_keep = []\n",
    "                ag_chain = row[\"antigen_chain\"]\n",
    "                ab_chain = row[\"antibody_chain\"]\n",
    "                for index_2, row_2 in df_output.iterrows():\n",
    "                    if row_2[\"interacting_entities\"] == \"INTRA_SELECTION\": #Do not included the interactions between or with water molecules. Note, we did not make a further selection, we look into all atoms, so INTER and INTRA_NON_SELECTION does not exist.\n",
    "                        if row_2[\"atom_bgn\"][0] == ag_chain:\n",
    "                            if row_2[\"atom_end\"][0] in ab_chain: #\"in\" such that works for full length antibodies\n",
    "                                indexes_to_keep.append(index_2)\n",
    "                        elif row_2[\"atom_end\"][0] in ab_chain:\n",
    "                            if row_2[\"atom_bgn\"][0] == ag_chain:\n",
    "                                indexes_to_keep.append(index_2)\n",
    "                df_filtered= df_output.iloc[indexes_to_keep, :].copy()\n",
    "                \n",
    "                ##Write all atom interacting information to file\n",
    "                if writing_file:\n",
    "                    print(\"WARNING: you are writing to a file\")\n",
    "                    df_filtered.to_csv(f\"{main_path}/output/{folder}/{pdb}_{ag_chain}_{ab_chain}_atom_interactions.csv\", index=False)\n",
    "                \n",
    "           \n",
    "                #Store interaction information per residue\n",
    "                df_residue_interaction = store_interactions_per_residue(df_filtered, columns_types_interactions)\n",
    "                \n",
    "                ##write all residue information to a file \n",
    "                if writing_file:\n",
    "                    print(\"WARNING: you are writing to a file\")\n",
    "                    df_residue_interaction.to_csv(f\"{main_path}/output/{folder}/{pdb}_{ag_chain}_{ab_chain}_residue_interactions.csv\", index=False)\n",
    "                \n",
    "                #Summarise result in one line\n",
    "                if len(ab_chain) == 2: #full length antibody\n",
    "                    df_residue_interaction[['chain_2', 'position_2']] = df_residue_interaction['residue_2'].str.split('/', expand=True)\n",
    "                    df_residue_interaction_heavy = df_residue_interaction[df_residue_interaction[\"chain_2\"] == ab_chain[0]]\n",
    "                    df_residue_interaction_light = df_residue_interaction[df_residue_interaction[\"chain_2\"] == ab_chain[1]]\n",
    "                    df_residue_interaction_heavy = df_residue_interaction_heavy.drop(['chain_2', 'position_2'], axis=1)\n",
    "                    df_residue_interaction_light = df_residue_interaction_light.drop(['chain_2', 'position_2'], axis=1)\n",
    "                    \n",
    "                    dict_summary_per_complex_heavy = store_interactions_residue_per_complex_dict(df_residue_interaction_heavy, columns_types_interactions, interaction_types_list, contact_types_list)\n",
    "                    dict_summary_per_complex_light = store_interactions_residue_per_complex_dict(df_residue_interaction_light, columns_types_interactions, interaction_types_list, contact_types_list)\n",
    "                    #Combine\n",
    "                    dict_summary_per_complex = combine_full_length_output(dict_summary_per_complex_heavy, dict_summary_per_complex_light, ab_chain[0], ab_chain[1])\n",
    "                else: #Single domain antibody\n",
    "                    dict_summary_per_complex = store_interactions_residue_per_complex_dict(df_residue_interaction, columns_types_interactions, interaction_types_list, contact_types_list)\n",
    "                \n",
    "                dict_summary_per_complex['pdb'] = pdb\n",
    "                dict_summary_per_complex['antigen_chain'] = ag_chain\n",
    "                dict_summary_per_complex['antibody_chain'] = ab_chain\n",
    "                df_line_info = pd.DataFrame(dict_summary_per_complex, index=[index_counter_main])\n",
    "                index_counter_main += 1\n",
    "                \n",
    "                #Update dataframe\n",
    "                df_store_interaction_per_dataset = pd.concat([df_store_interaction_per_dataset, df_line_info], ignore_index = True, axis = 0)\n",
    "        \n",
    "        #Failed arpeggio\n",
    "        except FileNotFoundError: #Arpeggio has not created the output file. Try cleaning the pdb\n",
    "            count_arpeggio_failed +=1\n",
    "            failed_arpeggio_pdbs.add(pdb)\n",
    "        except EmptyDataError: #The dataset is empty. Try cleaning the output file. \n",
    "            failed_arpeggio_pdbs.add(pdb)\n",
    "            count_arpeggio_failed +=1\n",
    "    \n",
    "    #Statement about the failed arpeggios\n",
    "    print(f\"Arpeggio could not run for {count_arpeggio_failed} PDBs\")\n",
    "    print(failed_arpeggio_pdbs)\n",
    "    \n",
    "    return df_store_interaction_per_dataset, failed_arpeggio_pdbs\n",
    "\n",
    "def store_interactions_per_residue(df_contacts, columns_types_interactions):\n",
    "    #this function summarise all the atom interacting information to residue interacting information. \n",
    "    #It loops over dataframes containing all information of the interacting between two residues. \n",
    "    #Per residues interacting it stores per type of interacting the amount of interactings that are made. So 2 means that two different atom combinations of the two residues are making this bond. \n",
    "    \n",
    "    columns_df = ['residue_1', 'residue_2', 'number_interactions'] + columns_types_interactions\n",
    "    df_residue_info = pd.DataFrame(columns=columns_df) \n",
    "    \n",
    "    #Split atom from chain and position\n",
    "    df_contacts[['chain_bgn', 'position_bgn', 'atom_type_bgn']] = df_contacts['atom_bgn'].str.split('/', expand=True)\n",
    "    df_contacts[\"chain_position_bng\"] = df_contacts['chain_bgn'] +\"/\"+ df_contacts[\"position_bgn\"] #Needed for full length\n",
    "    df_contacts[['chain_end', 'position_end', 'atom_type_end']] = df_contacts['atom_end'].str.split('/', expand=True)\n",
    "    df_contacts[\"chain_position_end\"] = df_contacts['chain_end'] +\"/\"+ df_contacts[\"position_end\"] #Needed for full length\n",
    "    \n",
    "    index_counter = 0\n",
    "    \n",
    "    #Select part dataframe for one bgn position\n",
    "    unique_positions_bgn_set = set(df_contacts['chain_position_bng'].tolist())\n",
    "    for unique_position_bgn in unique_positions_bgn_set:\n",
    "        df_selected_position = df_contacts.loc[df_contacts['chain_position_bng'] == unique_position_bgn]\n",
    "        \n",
    "        #Select part dataframe for on end position (so all atoms of two residues interacting)\n",
    "        unique_positions_bgn_end_set = set(df_selected_position['chain_position_end'].tolist())\n",
    "        for unique_position_end in unique_positions_bgn_end_set:\n",
    "            df_selected_interaction = df_selected_position.loc[df_contacts['chain_position_end'] == unique_position_end]\n",
    "            \n",
    "            #sum the types of interactions and add to dataframe \n",
    "            interaction_sum_serie=df_selected_interaction[columns_types_interactions].sum()\n",
    "            interaction_dict=interaction_sum_serie.to_dict()\n",
    "            interaction_dict[\"residue_1\"] = unique_position_bgn\n",
    "            interaction_dict[\"residue_2\"] = unique_position_end\n",
    "            interaction_dict[\"number_interactions\"] = df_selected_interaction.shape[0]\n",
    "\n",
    "            df_new_line = pd.DataFrame(interaction_dict, index=[index_counter])\n",
    "            df_residue_info = pd.concat([df_residue_info, df_new_line], ignore_index = True, axis = 0)\n",
    "            index_counter +=1\n",
    "    return df_residue_info\n",
    "            \n",
    "def store_interactions_residue_per_complex_dict(df_residue_interaction, columns_types_interactions, interaction_types_list, contact_types_list):\n",
    "    #This function stores the interaction of one antigen-antibody complex as one line in a dataframe\n",
    "    #For all types of bindings that are determined as interactions by \"columns_types_interactions\". It only counts how often it occurs between two residues. Not how many times it occurs within two residues. \n",
    "    #So for example if residues A-B make 3 times a hydrophilic bond (because 3 different atom combinations of these two residues make hydrophilic bonds) it is count as 1 in the summary file. \n",
    "    #Note, if these same A-B make also x times another bond this bond is also counted as 1. Therefore one residue pair can make multiple types of interactions \n",
    "\n",
    "    dict_interaction_res_occurence_complex = {}\n",
    "    for interaction_type in columns_types_interactions:\n",
    "        interaction_seen_between_residues_complex = df_residue_interaction[interaction_type].astype(bool).sum(axis=0)\n",
    "        dict_interaction_res_occurence_complex[interaction_type] = interaction_seen_between_residues_complex\n",
    "        \n",
    "    #Determine epitope, paratope, number interactions, number contacts and add this to the dictionary \n",
    "    dict_epitope, dict_paratope, number_interactions, number_contacts = store_epitope_paratope_dict(df_residue_interaction, interaction_types_list, contact_types_list)\n",
    "    dict_interaction_res_occurence_complex[\"epitope_interactions\"] = [dict_epitope]\n",
    "    dict_interaction_res_occurence_complex[\"paratope_interactions\"] = [dict_paratope]\n",
    "    dict_interaction_res_occurence_complex[\"number_interactions\"] = number_interactions\n",
    "    dict_interaction_res_occurence_complex[\"number_contacts\"] = number_contacts\n",
    "\n",
    "    return dict_interaction_res_occurence_complex\n",
    "\n",
    "def store_epitope_paratope_dict(df_residue_interaction, interaction_types_list, contact_types_list):\n",
    "    #This function set up the dataframes and calls the \"determine_interaction_dict()\" function in order to determine both the epitope and the paratope\n",
    "    df_residue_interaction[['chain_bgn', 'position_bgn']] = df_residue_interaction['residue_1'].str.split('/', expand=True)\n",
    "    df_residue_interaction[['chain_end', 'position_end']] = df_residue_interaction['residue_2'].str.split('/', expand=True)\n",
    "    \n",
    "    #antigen chain always in chain_bgn, antibody chain always in chain_end\n",
    "    dict_epitope_position, count_number_total_interactions_epi, count_number_total_contacts_epi = determine_interaction_dict(df_residue_interaction, interaction_types_list, contact_types_list, \"position_bgn\")\n",
    "    dict_paratope_position, count_number_total_interactions_para, count_number_total_contacts_para = determine_interaction_dict(df_residue_interaction, interaction_types_list, contact_types_list, \"position_end\")\n",
    "    \n",
    "    #Sanity check \n",
    "    if count_number_total_interactions_epi != count_number_total_interactions_para:\n",
    "        print(\"not the same interactions\")\n",
    "    if count_number_total_contacts_epi != count_number_total_contacts_para:\n",
    "        print(\"not the same contacts\")\n",
    "        \n",
    "    return dict_epitope_position, dict_paratope_position, count_number_total_interactions_epi, count_number_total_contacts_epi\n",
    "    \n",
    "def determine_interaction_dict(df_residue_interaction, interaction_types_list, contact_types_list, column_name):\n",
    "    #This function determines for every epitope (or paratope) residue how often it is interacting with another residue of the paratope (or epitope). Besides it counts the total amount of interactions within one complex.\n",
    "    #Note: now the residue can both form an interacting and a contact!!! Change to elif if we want to investigates the contacts. \n",
    "    \n",
    "    interaction_positions_set = set(df_residue_interaction[column_name].tolist())\n",
    "    dict_interaction_position = {}\n",
    "    count_number_total_interactions = 0\n",
    "    count_number_total_contacts = 0\n",
    "    \n",
    "    for interaction_pos in interaction_positions_set:\n",
    "        count_is_interacting = 0\n",
    "        df_selected_interaction_region = df_residue_interaction.loc[df_residue_interaction[column_name] == interaction_pos]\n",
    "        \n",
    "        for index, row in df_selected_interaction_region.iterrows():\n",
    "            if row[interaction_types_list].sum() >0:\n",
    "                #If one of the interaction types is seen, count it\n",
    "                count_is_interacting += 1\n",
    "                count_number_total_interactions +=1\n",
    "            if row[contact_types_list].sum() >0:\n",
    "                #If one of the contact types is seen, count it.\n",
    "                count_number_total_contacts += 1 \n",
    "        \n",
    "        #Save with how mamy residues the certain position is interacting. \n",
    "        if count_is_interacting > 0:\n",
    "            dict_interaction_position[interaction_pos] = count_is_interacting\n",
    "        \n",
    "    return dict_interaction_position, count_number_total_interactions, count_number_total_contacts\n",
    "       \n",
    "def combine_full_length_output(dict_heavy, dict_light, chain_heavy, chain_light):\n",
    "    #This function stores the information of the heavy and the light chain seperately and together. \n",
    "    dict_combined = {}\n",
    "    for key in dict_heavy.keys():\n",
    "        info_dict = {}\n",
    "        if isinstance(dict_heavy[key], list): #get dictionary out of the list\n",
    "            dict_heavy[key] = dict_heavy[key][0]\n",
    "            dict_light[key] = dict_light[key][0]\n",
    "        info_dict[chain_heavy] = dict_heavy[key]\n",
    "        info_dict[chain_light] = dict_light[key]\n",
    "        dict_combined[key] = [info_dict] #Brackets are needed to save it as one entry in the pdb. \n",
    "    return dict_combined\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ea034c-bf13-4da3-8293-2cdbac7a20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single domain antibodies\n",
    "ds_name_nb = \"Dataset_nb_filtered_num_interactions.csv\"\n",
    "df_interactions_nb = read_dataset(ds_name_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce887e3-46b3-4a46-b12b-af87cc986dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create_sh_arpeggio_commands(df_interactions_nb, start_command, folder_input, options_command, arpeggio_output_path, \"single_domain_arpeggio_commands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5876b902-f1b8-4c43-b826-95a0753c74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_summary_arpeggio_nb, failed_arpeggio_pdbs_nb = read_arpeggio_result(arpeggio_output_path, df_interactions_nb, columns_types_interactions, interaction_types, contact_types, \"single_domain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336b9139-0cfd-404d-b3cf-43c26d632661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary_arpeggio_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108d360b-df0f-4e57-ad79-270714046e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make script that does cleaning first on the failed once\n",
    "##create_sh_arpeggio_failed_pdbs(df_interactions_nb, failed_arpeggio_pdbs_nb, start_command, folder_input, options_command, file_cleaning, \"sd_arpeggio_failed_pdbs\")\n",
    "##create_py_arpeggio_failed_pdbs(df_interactions_nb, failed_arpeggio_pdbs_nb, start_command, folder_input, options_command, file_cleaning, \"sd_arpeggio_failed_pdbs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a996b1f-78b9-4c18-bd1a-b539b7e6fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Do for the failed once and concat results\n",
    "# df_summary_arpeggio_failed_nb, failed_twice_arpeggio_pdbs_nb = read_arpeggio_result(arpeggio_output_path, df_interactions_nb, columns_types_interactions, interaction_types, contact_types, \"single_domain\", failed_arpeggio_pdbs_nb)\n",
    "# print(f\"{len(failed_twice_arpeggio_pdbs_nb)} failed twice\")\n",
    "\n",
    "# # ##Combine dataframes\n",
    "# df_summary_arpeggio_total_nb = pd.concat([df_summary_arpeggio_nb, df_summary_arpeggio_failed_nb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508f9e61-dfbb-42d3-95a0-e76cc464672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary_arpeggio_total_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09dddc2b-4cbc-4e41-8655-0483b2dcff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #SAVE dataframe\n",
    "# df_summary_arpeggio_total_nb.to_csv(\"Dataset_nb_filtered_arpeggio.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae84cbde-07a8-4a05-a8e2-25244d36cfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: you are writing to a file\n",
      "WARNING: you are writing to a file\n",
      "Arpeggio could not run for 0 PDBs\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# #DO THIS FOR 7d2z, 6ui1, 5mhr\n",
    "# # #check specific pdb\n",
    "# df_input_test = df_interactions_nb[df_interactions_nb[\"pdb\"]==\"6ui1\"]\n",
    "# # failed_list = [\"6ui1\"]\n",
    "# failed_list = False\n",
    "# test_nb, failed_test = read_arpeggio_result(arpeggio_output_path, df_input_test, columns_types_interactions, interaction_types, contact_types, \"single_domain\", failed_list, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c9101f6-58dd-4fa0-a24a-c1c18fa8234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load created file\n",
    "ds_name_arp_nb = \"Dataset_nb_filtered_arpeggio.csv\"\n",
    "df_arp_nb = read_dataset(ds_name_arp_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3db272-483f-4d14-86a3-bf21c2f38a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aebbe59a-e7fe-4d17-84e9-c89f567611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full length antibodies\n",
    "ds_name_fv = \"Dataset_fv_filtered_num_interactions.csv\"\n",
    "df_interactions_fv = read_dataset(ds_name_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "552385f7-b3b7-4b89-83fa-754030ce937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create sh or py script that runs arpeggio automatically for all pdbs\n",
    "## create_sh_arpeggio_commands(df_interactions_fv, start_command, folder_input, options_command, arpeggio_output_path, \"full_length_arpeggio_unique_commands\")\n",
    "##create_py_arpeggio_commands(df_interactions_fv, start_command, folder_input, options_command, arpeggio_output_path, \"full_length_arpeggio_unique_commands\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2252f1a5-f62d-4bc9-8501-6e7517374281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arpeggio could not run for 284 PDBs\n",
      "{'7a3o', '3kr3', '3bn9', '5x2o', '2xqy', '4qhu', '6o39', '3zkn', '2aep', '7lfa', '3u7y', '3p0y', '5o14', '5wb9', '5o1r', '5bjz', '6glw', '5kvd', '6yla', '5vag', '6ml8', '4i77', '7bej', '6azz', '6erx', '6h2y', '4xwo', '4uta', '3l5w', '7neg', '7dm2', '6nmt', '5tfw', '6d2p', '7dr4', '6vmj', '4xvu', '6osv', '5vjq', '7djz', '3wkm', '6al0', '3wih', '4jlr', '6was', '7jmp', '6bit', '2vxt', '3lh2', '3se8', '4zff', '5ugy', '5b3j', '2ypv', '7lsf', '3liz', '5usi', '7msq', '4ogy', '5bk1', '7vux', '7mzj', '6ohg', '5th9', '7dc8', '7kmi', '5d70', '7ps4', '4lsu', '6ddv', '6j14', '4lvh', '7mzh', '6o3a', '2w9e', '4cni', '7nx7', '4zfg', '3tje', '6jep', '6cxy', '7kf0', '4hc1', '6wzl', '7n3d', '3d85', '5vkd', '7ps2', '4uu9', '5mes', '4bz1', '6osh', '7jx3', '7s4s', '7bz5', '6wzm', '5tzt', '6ddm', '7or9', '4ot1', '7np1', '4xmp', '7kmg', '6hf1', '7lm9', '6lyn', '1yqv', '5f96', '6tyb', '5tzu', '4jkp', '6cbv', '6ddr', '7coe', '7ps1', '6a67', '4d9r', '6b0h', '6iea', '4i3r', '4o9h', '6t9d', '6wgl', '3l95', '7qu2', '3l5y', '3nps', '6xkp', '5tud', '7kfx', '3skj', '6xxv', '2xtj', '7mzk', '7lfb', '4z7n', '6mtq', '7phw', '3rvw', '5k9q', '6fax', '6pe8', '6o9h', '7kez', '3thm', '3lev', '4yue', '7bei', '6o1f', '7kpb', '6bfq', '1ob1', '7kpg', '7mf1', '4dn4', '7dm1', '4ht1', '7orb', '5dur', '3pnw', '4liq', '7seg', '5ies', '3hi6', '4hlz', '4bz2', '4zs7', '6mlk', '3so3', '5ob5', '7kd6', '2oz4', '4aei', '6wh9', '4rwy', '2ny3', '7ce2', '4hcr', '5d96', '5e94', '7lm8', '6vy4', '6iut', '5en2', '6aod', '6p4b', '4ydk', '6gku', '6oz2', '7qu1', '5w4l', '6yio', '7qny', '5w06', '7beh', '7kmh', '7bep', '4dtg', '4irz', '3sqo', '4m62', '6wzk', '4py8', '5l6y', '5d71', '5k9k', '5w5z', '5kvf', '5kjr', '7e3o', '6a77', '7mzf', '7e9b', '6fxn', '4cmh', '7bel', '4lsq', '7kf1', '7lr3', '4xph', '7ket', '6svl', '6pzf', '5otj', '5vjo', '7mmo', '6z2m', '7b3o', '4ogx', '7mrz', '5nuz', '7tn0', '3ma9', '6o9i', '4rrp', '5f3h', '4y5v', '3mxw', '5d72', '5kvg', '6mft', '6a3w', '6ss4', '5nmv', '6s5a', '6vel', '2r0l', '6icc', '5ngv', '6ayz', '7n4i', '5ldn', '3pgf', '7cho', '4lvn', '5vpl', '2vdm', '4kht', '5vyf', '4etq', '6b0g', '6okm', '5utz', '7kfv', '5bk2', '4qww', '7kql', '5te4', '6dkj', '1g7i', '4xvt', '7lcv', '7n3c', '6r8x', '7s0b', '6u6u', '1e6j', '6yax', '5lqb', '7kyo', '7cjf', '7lop', '5mo9', '7lr4'}\n"
     ]
    }
   ],
   "source": [
    "# df_summary_arpeggio_fv, failed_arpeggio_pdbs_fv = read_arpeggio_result(arpeggio_output_path, df_interactions_fv, columns_types_interactions, interaction_types, contact_types, \"full_length\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c164f06e-2ca9-4e09-b3b1-7a57b5460b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary_arpeggio_fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d23860ce-4b0b-493e-8c7c-43796d7f4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the failed once first and run again\n",
    "#create_py_arpeggio_failed_pdbs(df_interactions_fv, failed_arpeggio_pdbs_fv, start_command, folder_input, options_command, file_cleaning, \"fv_arpeggio_failed_pdbs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6642d5bb-0046-4309-af9d-a445997f1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arpeggio could not run for 2 PDBs\n",
      "{'4xwo', '5k9q'}\n",
      "2 failed twice\n"
     ]
    }
   ],
   "source": [
    "# # # #Do for the failed once and concat results\n",
    "# df_summary_arpeggio_failed_fv, failed_twice_arpeggio_pdbs_fv = read_arpeggio_result(arpeggio_output_path, df_interactions_fv, columns_types_interactions, interaction_types, contact_types, \"full_length\", failed_arpeggio_pdbs_fv)\n",
    "# print(f\"{len(failed_twice_arpeggio_pdbs_fv)} failed twice\")\n",
    "\n",
    "# #Combine dataframes\n",
    "# df_summary_arpeggio_total_fv = pd.concat([df_summary_arpeggio_fv, df_summary_arpeggio_failed_fv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d84993c-fcd6-4482-8190-7367efa77334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary_arpeggio_total_fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18c97ac9-e428-419c-a58f-dff3a586e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save information to files\n",
    "# df_summary_arpeggio_total_fv.to_csv(\"Dataset_fv_filtered_arpeggio.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4de12b-e034-4daa-8eb2-307c277ae9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "6583c3f449b2025ea649b72d4a52efb1871eb538dba3514b6dae43f01b685c61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
